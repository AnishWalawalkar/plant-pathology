{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from src.plant_pathology.leaf_dataset import LeafDataset\n",
    "from src.plant_pathology.model_loops import training, validation, testing\n",
    "from src.plant_pathology.models import get_resnet, get_densenet, get_effecientnet\n",
    "from src.plant_pathology.visualizations import show_saliency_maps, create_class_visualization\n",
    "from src.plant_pathology.loss import LabelSmoothingCrossEntropy\n",
    "from src.plant_pathology.metrics import comp_metric, healthy_roc_auc, multiple_diseases_roc_auc, scab_roc_auc, rust_roc_auc\n",
    "from src.plant_pathology.onecyclelr import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = Path('./plant-pathology-2020-fgvc7/images')\n",
    "\n",
    "def image_path(file_stem):\n",
    "    return IMAGE_PATH/f'{file_stem}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./plant-pathology-2020-fgvc7/train.csv')\n",
    "test_df = pd.read_csv('./plant-pathology-2020-fgvc7/test.csv')\n",
    "\n",
    "train_paths = train_df['img_file'] = train_df['image_id'].apply(image_path)\n",
    "test_paths = test_df['img_file'] = test_df['image_id'].apply(image_path)\n",
    "\n",
    "train_labels = train_df[['healthy','multiple_diseases','rust','scab']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size = 0.2, random_state=23, stratify = train_labels)\n",
    "train_paths.reset_index(drop=True,inplace=True)\n",
    "train_labels.reset_index(drop=True,inplace=True)\n",
    "valid_paths.reset_index(drop=True,inplace=True)\n",
    "valid_labels.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scab = cv2.imread(str(train_df.iloc[0]['img_file']))\n",
    "multi = cv2.imread(str(train_df.iloc[1]['img_file']))\n",
    "rust = cv2.imread(str(train_df.iloc[1819]['img_file']))\n",
    "healthy = cv2.imread(str(train_df.iloc[4]['img_file']))\n",
    "kernel = np.ones((6,6),np.float32)/25\n",
    "\n",
    "types = [healthy, multi, rust, scab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 5)\n",
    "y_labels = ['Healthy', 'Multi', 'Rust', 'Scab']\n",
    "x_labels = ['Normal', 'Horizontal Flip', 'Vertical Flip', 'Rotated 25', 'Filtered']\n",
    "\n",
    "for i in range(4):\n",
    "    axs[i, 0].imshow(types[i])\n",
    "    axs[i, 0].set(ylabel=y_labels[i])\n",
    "    axs[i, 1].imshow(cv2.flip(types[i], 1))\n",
    "    axs[i, 2].imshow(cv2.flip(types[i], 0))\n",
    "    axs[i, 3].imshow(imutils.rotate(types[i], 25))\n",
    "    axs[i, 4].imshow(cv2.filter2D(types[i],-1,kernel))\n",
    "    \n",
    "    if (i + 1) == 4:\n",
    "        for j in range(5):\n",
    "            axs[i, j].set(xlabel=x_labels[j])\n",
    "    \n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "        \n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('example.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 30\n",
    "TRAIN_SIZE = train_labels.shape[0]\n",
    "VALID_SIZE = valid_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeafDataset(train_paths, train_labels)\n",
    "trainloader = Data.DataLoader(train_dataset, shuffle=True, batch_size = BATCH_SIZE, num_workers = 2)\n",
    "\n",
    "valid_dataset = LeafDataset(valid_paths, valid_labels, train = False)\n",
    "validloader = Data.DataLoader(valid_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)\n",
    "\n",
    "test_dataset = LeafDataset(test_paths,train = False, test = True)\n",
    "testloader = Data.DataLoader(test_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_accuracy_score(labels, preds):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return accuracy_score(labels, preds)\n",
    "acc_fns = [modified_accuracy_score, healthy_roc_auc, multiple_diseases_roc_auc, rust_roc_auc, scab_roc_auc, comp_metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please only run one of the net sections before running the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get models\n",
    "model selections:\n",
    "- densenet\n",
    "- resnet\n",
    "- inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = get_densenet(train_labels)\n",
    "model = densenet\n",
    "num_params = len(list(model.parameters()))\n",
    "for idx, param  in enumerate(model.parameters()):\n",
    "    if idx < num_params // 2:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = get_resnet(train_labels, pretrained=True)\n",
    "model = resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EffecientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effecient_net = get_effecientnet(train_labels, pretrained=True)\n",
    "model = effecient_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=8e-4, momentum=0.8, weight_decay=1e-4)\n",
    "loss_fn = LabelSmoothingCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = LRFinder(model, optimizer, loss_fn, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.range_test(trainloader, end_lr=100, num_iter=100, step_mode='exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Optimizer and Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 6\n",
    "end_lr = 4 / 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=end_lr, momentum=0.8, weight_decay=1e-4)\n",
    "\n",
    "num_iters = len(train_dataset) / BATCH_SIZE * NUM_EPOCHS\n",
    "scheduler = OneCycleLR(optimizer, num_steps=num_iters // 2, lr_range=(end_lr/factor, end_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "lrs = []\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = get_resnet(train_labels, pretrained=True)\n",
    "best_combined_roc_auc = float('-inf')\n",
    "\n",
    "# now = datetime.now().strftime(\"%m_%d_%Y\")\n",
    "Path(f'model_checkpoints/{model.__class__.__name__}').mkdir(parents=True, exist_ok=True)\n",
    "checkpoint = Path(f'model_checkpoints/{model.__class__.__name__}')\n",
    "# checkpoint.mkdir(exist_ok=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    tl, ta, lr = training(model, trainloader, optimizer, scheduler, loss_fn, acc_fns, device, TRAIN_SIZE)\n",
    "    display(pd.DataFrame([epoch, tl, *ta], index=['Epoch', 'Train Loss', 'Train Accuracy',  'Healthy ROC_AUC', 'Multi ROC_AUC', 'Rust ROC_AUC', 'Scab ROC_AUC', 'Combined ROC_AUC']).T)\n",
    "    vl, va, conf_mat = validation(model, validloader, loss_fn, acc_fns, confusion_matrix, device, VALID_SIZE)\n",
    "    display(pd.DataFrame([epoch, vl, *va], index=['Epoch', 'Valid Loss', 'Valid Accuracy',  'Healthy ROC_AUC', 'Multi ROC_AUC', 'Rust ROC_AUC', 'Scab ROC_AUC', 'Combined ROC_AUC']).T)\n",
    "    train_loss.append(tl)\n",
    "    valid_loss.append(vl)\n",
    "    train_acc.append(ta)\n",
    "    val_acc.append(va)\n",
    "    lrs.extend(lr)\n",
    "    \n",
    "    if va[-1] > best_combined_roc_auc:\n",
    "        best_combined_roc_auc = va[-1]\n",
    "        best_model.load_state_dict(model.state_dict()) # copy weights and stuff\n",
    "    \n",
    "    if (epoch+1)%10==0:\n",
    "        torch.save(model.state_dict(), checkpoint/f'epoch_{epoch}_{loss_fn.__class__.__name__}.pt')\n",
    "        \n",
    "torch.save(best_model.state_dict(), checkpoint/f'best_{loss_fn.__class__.__name__}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = np.array(train_acc)\n",
    "val_acc = np.array(val_acc)\n",
    "lrs = np.array(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame(train_acc)\n",
    "train_results['Loss'] = train_loss\n",
    "train_results.columns = ['Accuracy', 'Healthy ROC_AUC', 'Multi ROC_AUC', 'Rust ROC_AUC', 'Scab ROC_AUC', 'Combined ROC_AUC', 'Loss']\n",
    "train_results.index.name = 'Epoch'\n",
    "\n",
    "val_results = pd.DataFrame(val_acc)\n",
    "val_results['Loss'] = valid_loss\n",
    "val_results.columns = ['Accuracy', 'Healthy ROC_AUC', 'Multi ROC_AUC', 'Rust ROC_AUC', 'Scab ROC_AUC', 'Combined ROC_AUC', 'Loss']\n",
    "val_results.index.name = 'Epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'model_results/{best_model.__class__.__name__}').mkdir(parents=True, exist_ok=True)\n",
    "results_folder = Path(f'model_results/{best_model.__class__.__name__}')\n",
    "train_results.to_csv(results_folder/f'{model.__class__.__name__}_train_results_{loss_fn.__class__.__name__}.csv')\n",
    "val_results.to_csv(results_folder/f'{model.__class__.__name__}_valid_results_{loss_fn.__class__.__name__}.csv')\n",
    "np.savetxt(results_folder/f'{model.__class__.__name__}_lrs.csv', lrs, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'model_plots/{best_model.__class__.__name__}').mkdir(parents=True, exist_ok=True)\n",
    "plots_folder = Path(f'model_plots/{best_model.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(list(range(len(lrs))), lrs)\n",
    "plt.xlabel('# Iterations', fontsize=18)\n",
    "plt.ylabel('Learning Rate', fontsize=18)\n",
    "plt.title('Learning Rate vs # Iterations', fontsize=20)\n",
    "plt.savefig(plots_folder/'learning_rate.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.ylim(0,1.5)\n",
    "sns.lineplot(list(range(len(train_loss))), train_loss)\n",
    "sns.lineplot(list(range(len(valid_loss))), valid_loss)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.legend(['Train','Val'], fontsize=16)\n",
    "plt.title('Loss vs Epoch', fontsize=20)\n",
    "plt.savefig(plots_folder/'loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_names = ['Accuracy', 'Healthy ROC_AUC', 'Multi ROC_AUC', 'Rust ROC_AUC', 'Scab ROC_AUC', 'Combined ROC_AUC']\n",
    "for idx, acc_name in enumerate(acc_names):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.lineplot(list(range(len(train_acc[:, idx]))), train_acc[:, idx])\n",
    "    sns.lineplot(list(range(len(val_acc[:, idx]))), val_acc[:, idx])\n",
    "    plt.xlabel('Epoch', fontsize=18)\n",
    "    plt.ylabel(acc_name, fontsize=18)\n",
    "    plt.legend(['Train','Val'], fontsize=16)\n",
    "    plt.title(f'{acc_name} vs Epoch', fontsize=20)\n",
    "    plt.savefig(plots_folder/f'{model.__class__.__name__}_{acc_name}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, va, conf_mat = validation(best_model, validloader, loss_fn, acc_fns, confusion_matrix, device, VALID_SIZE)\n",
    "labels = ['Healthy', 'Multiple','Rust','Scab']\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.savefig(plots_folder/'confusion.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_output(model, device):\n",
    "    model = model.to(device)\n",
    "    subs = []\n",
    "    for i in range(5): #average over 5 runs\n",
    "        out = testing(model, testloader, device)\n",
    "        output = pd.DataFrame(softmax(out,1), columns = ['healthy','multiple_diseases','rust','scab']) #the submission expects probability scores for each class\n",
    "        output.drop(0, inplace = True)\n",
    "        output.reset_index(drop=True,inplace=True)\n",
    "        subs.append(output)\n",
    "\n",
    "    sub_eff1 = sum(subs)/5\n",
    "    return sub_eff1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Emsembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_checkpoint = get_densenet(train_labels, model_path='/home/anishwalawalkar/plant-pathology/model_checkpoints/DenseNet/best_LabelSmoothingCrossEntropy.pt')\n",
    "# resnet_checkpoint = get_resnet(train_labels, model_path='/home/anishwalawalkar/plant-pathology/model_checkpoints/ResNet_best_smoothing.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_densenet = get_testing_output(densenet_checkpoint, device)\n",
    "# sub_resnet = get_testing_output(resnet_checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = 0.25 * sub_resnet + 0.75 * sub_densenet\n",
    "submission = sub_densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['image_id'] = test_df['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_checkpoint = get_densenet(train_labels, model_path='/home/anishwalawalkar/plant-pathology/model_checkpoints/DenseNet/best_LabelSmoothingCrossEntropy.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Healthy', 'Multi', 'Rust', 'Scab']\n",
    "y = [0, 1, 2, 3]\n",
    "X = np.array([\n",
    "    np.array(Image.open(str(train_df.iloc[1817]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[1]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[1819]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[0]['img_file']))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_saliency_maps(X, y, densenet_checkpoint, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_checkpoint = get_densenet(train_labels, model_path='/home/anishwalawalkar/plant-pathology/model_checkpoints/DenseNet/best_LabelSmoothingCrossEntropy.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_class_visualization(y[3], model, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
