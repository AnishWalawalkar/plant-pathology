{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from src.plant_pathology.leaf_dataset import LeafDataset\n",
    "from src.plant_pathology.model_loops import training, validation, testing\n",
    "from src.plant_pathology.models import get_resnet, get_densenet, get_effecientnet\n",
    "from src.plant_pathology.visualizations import show_saliency_maps, create_class_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = Path('./plant-pathology-2020-fgvc7/images')\n",
    "\n",
    "def image_path(file_stem):\n",
    "    return IMAGE_PATH/f'{file_stem}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./plant-pathology-2020-fgvc7/train.csv')\n",
    "test_df = pd.read_csv('./plant-pathology-2020-fgvc7/test.csv')\n",
    "\n",
    "train_paths = train_df['img_file'] = train_df['image_id'].apply(image_path)\n",
    "test_paths = test_df['img_file'] = test_df['image_id'].apply(image_path)\n",
    "\n",
    "train_labels = train_df[['healthy','multiple_diseases','rust','scab']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size = 0.2, random_state=23, stratify = train_labels)\n",
    "train_paths.reset_index(drop=True,inplace=True)\n",
    "train_labels.reset_index(drop=True,inplace=True)\n",
    "valid_paths.reset_index(drop=True,inplace=True)\n",
    "valid_labels.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scab = cv2.imread(str(train_df.iloc[0]['img_file']))\n",
    "multi = cv2.imread(str(train_df.iloc[1]['img_file']))\n",
    "rust = cv2.imread(str(train_df.iloc[1819]['img_file']))\n",
    "healthy = cv2.imread(str(train_df.iloc[4]['img_file']))\n",
    "kernel = np.ones((6,6),np.float32)/25\n",
    "\n",
    "types = [healthy, multi, rust, scab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 5)\n",
    "y_labels = ['Healthy', 'Multi', 'Rust', 'Scab']\n",
    "x_labels = ['Normal', 'Horizontal Flip', 'Vertical Flip', 'Rotated 25', 'Filtered']\n",
    "\n",
    "for i in range(4):\n",
    "    axs[i, 0].imshow(types[i])\n",
    "    axs[i, 0].set(ylabel=y_labels[i])\n",
    "    axs[i, 1].imshow(cv2.flip(types[i], 1))\n",
    "    axs[i, 2].imshow(cv2.flip(types[i], 0))\n",
    "    axs[i, 3].imshow(imutils.rotate(types[i], 25))\n",
    "    axs[i, 4].imshow(cv2.filter2D(types[i],-1,kernel))\n",
    "    \n",
    "    if (i + 1) == 4:\n",
    "        for j in range(5):\n",
    "            axs[i, j].set(xlabel=x_labels[j])\n",
    "    \n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "        \n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('example.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 30\n",
    "TRAIN_SIZE = train_labels.shape[0]\n",
    "VALID_SIZE = valid_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeafDataset(train_paths, train_labels)\n",
    "trainloader = Data.DataLoader(train_dataset, shuffle=True, batch_size = BATCH_SIZE, num_workers = 2)\n",
    "\n",
    "valid_dataset = LeafDataset(valid_paths, valid_labels, train = False)\n",
    "validloader = Data.DataLoader(valid_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)\n",
    "\n",
    "test_dataset = LeafDataset(test_paths,train = False, test = True)\n",
    "testloader = Data.DataLoader(test_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please only run one of the net sections before running the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = get_densenet(train_labels)\n",
    "model = densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(densenet.parameters(), lr=8e-4, weight_decay = 1e-3)\n",
    "num_train_steps = int(len(train_dataset) / BATCH_SIZE * NUM_EPOCHS)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=len(train_dataset)/BATCH_SIZE*5, num_training_steps=num_train_steps)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = get_resnet(train_labels)\n",
    "model = resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(densenet.parameters(), lr=8e-4, weight_decay = 1e-3)\n",
    "num_train_steps = int(len(train_dataset) / BATCH_SIZE * NUM_EPOCHS)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=len(train_dataset)/BATCH_SIZE*5, num_training_steps=num_train_steps)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(densenet.parameters(), lr=8e-4, weight_decay = 1e-3)\n",
    "num_train_steps = int(len(train_dataset) / BATCH_SIZE * NUM_EPOCHS)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=len(train_dataset)/BATCH_SIZE*5, num_training_steps=num_train_steps)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    tl, ta = training(model, trainloader, optimizer, scheduler, loss_fn, accuracy_score, TRAIN_SIZE)\n",
    "    vl, va, conf_mat = validation(model, validloader, loss_fn, accuracy_score, confusion_matrix, VALID_SIZE)\n",
    "    train_loss.append(tl)\n",
    "    valid_loss.append(vl)\n",
    "    train_acc.append(ta)\n",
    "    val_acc.append(va)\n",
    "    \n",
    "    if (epoch+1)%10==0:\n",
    "        checkpoint = Path('model_checkpoints/')\n",
    "        checkpoint.mkdir(exist_ok=True)\n",
    "        torch.save(model.state_dict(), checkpoint/f'epoch_{epoch}.pt')\n",
    "    \n",
    "    printstr = 'Epoch: '+ str(epoch) + ', Train loss: ' + str(tl) + ', Val loss: ' + str(vl) + ', Train acc: ' + str(ta) + ', Val acc: ' + str(va)\n",
    "    tqdm.write(printstr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.ylim(0,1.5)\n",
    "sns.lineplot(list(range(len(train_loss))), train_loss)\n",
    "sns.lineplot(list(range(len(valid_loss))), valid_loss)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.legend(['Train','Val'], fontsize=16)\n",
    "plt.title('Loss vs Epoch', fontsize=20)\n",
    "plt.savefig('densenet_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(list(range(len(train_acc))), train_acc)\n",
    "sns.lineplot(list(range(len(val_acc))), val_acc)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.legend(['Train','Val'], fontsize=16)\n",
    "plt.title('Loss vs Epoch', fontsize=20)\n",
    "plt.savefig('densenet_accuracy.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Healthy', 'Multiple','Rust','Scab']\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.savefig('confusion.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = []\n",
    "for i in range(5): #average over 5 runs\n",
    "    out = testing(model, testloader)\n",
    "    output = pd.DataFrame(softmax(out,1), columns = ['healthy','multiple_diseases','rust','scab']) #the submission expects probability scores for each class\n",
    "    output.drop(0, inplace = True)\n",
    "    output.reset_index(drop=True,inplace=True)\n",
    "    subs.append(output)\n",
    "\n",
    "sub_eff1 = sum(subs)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Emsembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_densenet(train_labels, model_path='/Users/anishwalawalkar/Repositories/cs231n/plant-pathology/model_checkpoints/densenet_epoch29.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Healthy', 'Multi', 'Rust', 'Scab']\n",
    "y = [0, 1, 2, 3]\n",
    "X = np.array([\n",
    "    np.array(Image.open(str(train_df.iloc[1817]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[1]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[1819]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[0]['img_file']))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_saliency_maps(X, y, model, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_class_visualization(y[0], model, torch.FloatTensor, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
