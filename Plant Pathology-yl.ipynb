{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python\n",
    "# !pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from scipy.special import softmax\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torchvision import transforms as T, models\n",
    "from torchsummary import summary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plant_pathology.leaf_dataset import LeafDataset\n",
    "from src.plant_pathology.model_loops import training, validation, testing\n",
    "from src.plant_pathology.models import get_resnet, get_densenet, get_effecientnet\n",
    "from src.plant_pathology.visualizations import show_saliency_maps, create_class_visualization\n",
    "from src.plant_pathology.models import initialize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./plant-pathology-2020-fgvc7/\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception, effecientnet]\n",
    "model_name = \"effecientnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 4\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 2\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = Path(data_dir +'images')\n",
    "\n",
    "def image_path(file_stem):\n",
    "    return IMAGE_PATH/f'{file_stem}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir+'train.csv')\n",
    "test_df = pd.read_csv(data_dir + 'test.csv')\n",
    "\n",
    "train_paths = train_df['img_file'] = train_df['image_id'].apply(image_path)\n",
    "test_paths = test_df['img_file'] = test_df['image_id'].apply(image_path)\n",
    "\n",
    "train_labels = train_df[['healthy','multiple_diseases','rust','scab']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size = 0.2, random_state=23, stratify = train_labels)\n",
    "train_paths.reset_index(drop=True,inplace=True)\n",
    "train_labels.reset_index(drop=True,inplace=True)\n",
    "valid_paths.reset_index(drop=True,inplace=True)\n",
    "valid_labels.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scab = cv2.imread(str(train_df.iloc[0]['img_file']))\n",
    "multi = cv2.imread(str(train_df.iloc[1]['img_file']))\n",
    "rust = cv2.imread(str(train_df.iloc[1819]['img_file']))\n",
    "healthy = cv2.imread(str(train_df.iloc[4]['img_file']))\n",
    "kernel = np.ones((6,6),np.float32)/25\n",
    "\n",
    "types = [healthy, multi, rust, scab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 5)\n",
    "y_labels = ['Healthy', 'Multi', 'Rust', 'Scab']\n",
    "x_labels = ['Normal', 'Horizontal Flip', 'Vertical Flip', 'Rotated 25', 'Filtered']\n",
    "\n",
    "for i in range(4):\n",
    "    axs[i, 0].imshow(types[i])\n",
    "    axs[i, 0].set(ylabel=y_labels[i])\n",
    "    axs[i, 1].imshow(cv2.flip(types[i], 1))\n",
    "    axs[i, 2].imshow(cv2.flip(types[i], 0))\n",
    "    axs[i, 3].imshow(imutils.rotate(types[i], 25))\n",
    "    axs[i, 4].imshow(cv2.filter2D(types[i],-1,kernel))\n",
    "    \n",
    "    if (i + 1) == 4:\n",
    "        for j in range(5):\n",
    "            axs[i, j].set(xlabel=x_labels[j])\n",
    "    \n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "        \n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('example.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = train_labels.shape[0]\n",
    "VALID_SIZE = valid_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeafDataset(train_paths, train_labels)\n",
    "trainloader = Data.DataLoader(train_dataset, shuffle=True, batch_size = batch_size, num_workers = 2)\n",
    "\n",
    "valid_dataset = LeafDataset(valid_paths, valid_labels, train = False)\n",
    "validloader = Data.DataLoader(valid_dataset, shuffle=False, batch_size = batch_size, num_workers = 2)\n",
    "\n",
    "test_dataset = LeafDataset(test_paths,train = False, test = True)\n",
    "testloader = Data.DataLoader(test_dataset, shuffle=False, batch_size = batch_size, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_f1 = partial(f1_score, labels=[0, 1, 2, 3], average='weighted')\n",
    "macro_f1 = partial(f1_score, labels=[0, 1, 2, 3], average='macro')\n",
    "micro_f1 = partial(f1_score, labels=[0, 1, 2, 3], average='micro')\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "# micro_roc_auc_score = partial(roc_auc_score, labels=[0, 1, 2, 3], average='macro', multi_class='ovr')\n",
    "# acc_fns = [accuracy_score, weighted_f1, macro_f1, micro_f1, roc_auc_score]\n",
    "acc_fns = [accuracy_score, weighted_f1, macro_f1, micro_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer_ft = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=8e-4, weight_decay = 1e-3)\n",
    "num_train_steps = int(len(train_dataset) / batch_size * num_epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer_ft, num_warmup_steps=len(train_dataset)/batch_size*5, num_training_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    tl, ta = training(model_ft, trainloader, optimizer_ft, scheduler, loss_fn, acc_fns, device, TRAIN_SIZE)\n",
    "    vl, va, conf_mat = validation(model_ft, validloader, loss_fn, acc_fns, confusion_matrix, device, VALID_SIZE)\n",
    "    train_loss.append(tl)\n",
    "    valid_loss.append(vl)\n",
    "    train_acc.append(ta)\n",
    "    val_acc.append(va)\n",
    "    \n",
    "    if (epoch+1)%10==0:\n",
    "        checkpoint = Path('model_checkpoints/')\n",
    "        checkpoint.mkdir(exist_ok=True)\n",
    "        torch.save(model.state_dict(), checkpoint/f'{model.__class__.__name__}_epoch_{epoch}.pt')\n",
    "    \n",
    "    printstr = f'Epoch: {epoch} , Train loss: {round(tl, 3)}, Val loss: {round(vl, 3)}, Train accs: {[round(t_a, 3) for t_a in ta]}, Val accs: {[round(v_a, 3) for v_a in va]}'\n",
    "    tqdm.write(printstr)\n",
    "    \n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = np.array(train_acc)\n",
    "val_acc = np.array(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame(train_acc)\n",
    "train_results[4] = train_loss\n",
    "train_results.columns = ['Accuracy', 'F1 Weighted', 'F1 Macro', 'F1 Micro', 'Loss']\n",
    "train_results.index.name = 'Epoch'\n",
    "\n",
    "val_results = pd.DataFrame(val_acc)\n",
    "val_results[4] = valid_loss\n",
    "val_results.columns = ['Accuracy', 'F1 Weighted', 'F1 Macro', 'F1 Micro', 'Loss']\n",
    "val_results.index.name = 'Epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results.to_csv(f'model_results/{model_ft.__class__.__name__}_train_results.csv')\n",
    "val_results.to_csv(f'model_results/{model_ft.__class__.__name__}_valid_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.ylim(0,1.5)\n",
    "sns.lineplot(list(range(len(train_loss))), train_loss)\n",
    "sns.lineplot(list(range(len(valid_loss))), valid_loss)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.legend(['Train','Val'], fontsize=16)\n",
    "plt.title('Loss vs Epoch', fontsize=20)\n",
    "plt.savefig(f'{model_ft.__class__.__name__}_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_names = ['Accuracy', 'F1 Weighted', 'F1 Macro', 'F1 Micro']\n",
    "for idx, acc_name in enumerate(acc_names):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.lineplot(list(range(len(train_acc[:, idx]))), train_acc[:, idx])\n",
    "    sns.lineplot(list(range(len(val_acc[:, idx]))), val_acc[:, idx])\n",
    "    plt.xlabel('Epoch', fontsize=18)\n",
    "    plt.ylabel(acc_name, fontsize=18)\n",
    "    plt.legend(['Train','Val'], fontsize=16)\n",
    "    plt.title(f'{acc_name} vs Epoch', fontsize=20)\n",
    "    plt.savefig(f'{model_ft.__class__.__name__}_accuracy.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = ['Healthy', 'Multiple','Rust','Scab']\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.savefig(f'{model_ft.__class__.__name__}_confusion.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_output(model, device):\n",
    "    model = model.to(device)\n",
    "    subs = []\n",
    "    for i in range(5): #average over 5 runs\n",
    "        out = testing(model, testloader, device)\n",
    "        output = pd.DataFrame(softmax(out,1), columns = ['healthy','multiple_diseases','rust','scab']) #the submission expects probability scores for each class\n",
    "        output.drop(0, inplace = True)\n",
    "        output.reset_index(drop=True,inplace=True)\n",
    "        subs.append(output)\n",
    "\n",
    "    sub_eff1 = sum(subs)/5\n",
    "    return sub_eff1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Emsembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_checkpoint = get_densenet(train_labels, model_path='./model_checkpoints/DenseNet_epoch_29.pt')\n",
    "resnet_checkpoint = get_resnet(train_labels, model_path='./model_checkpoints/ResNet_epoch_29.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_densenet = get_testing_output(densenet_checkpoint, device)\n",
    "sub_resnet = get_testing_output(resnet_checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = (sub_densenet + sub_resnet)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['image_id'] = test_df['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_densenet(train_labels, model_path='./model_checkpoints/DenseNet_epoch_29.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Healthy', 'Multi', 'Rust', 'Scab']\n",
    "y = [0, 1, 2, 3]\n",
    "X = np.array([\n",
    "    np.array(Image.open(str(train_df.iloc[1817]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[1]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[1819]['img_file']))),\n",
    "    np.array(Image.open(str(train_df.iloc[0]['img_file']))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_saliency_maps(X, y, model, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_densenet(train_labels, model_path='./model_checkpoints/DenseNet_epoch_29.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_class_visualization(y[3], model, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
